#!/usr/bin/env python3
"""Debug and introspection tool for decision models.

This script provides detailed introspection and debugging capabilities
for decision models generated by the multi-pass analyzer.

Usage:
    python scripts/debug_decision_model.py MODEL_FILE
    python scripts/debug_decision_model.py --analyze FILE [--output-dir DIR]

Copyright (c) 2025 Jim Schilling
This software is released under the MIT License.
"""

import argparse
import json
from pathlib import Path

from splurge_unittest_to_pytest.decision_model import DecisionModel


def load_decision_model(file_path: Path) -> DecisionModel | None:
    """Load a decision model from a JSON file."""
    try:
        with open(file_path, encoding="utf-8") as f:
            data = json.load(f)

        # Reconstruct the decision model from JSON
        decision_model = DecisionModel(module_proposals={})

        for _module_name, module_data in data.get("module_proposals", {}).items():
            # This is a simplified reconstruction - in practice we'd want more robust deserialization
            from splurge_unittest_to_pytest.decision_model import ClassProposal, FunctionProposal, ModuleProposal

            module_prop = ModuleProposal(
                module_name=module_data["module_name"],
                class_proposals={},
                module_fixtures=module_data.get("module_fixtures", []),
                module_imports=module_data.get("module_imports", []),
                top_level_assignments=module_data.get("top_level_assignments", {}),
            )

            for _class_name, class_data in module_data.get("class_proposals", {}).items():
                class_prop = ClassProposal(class_name=class_data["class_name"], function_proposals={})

                for _func_name, func_data in class_data.get("function_proposals", {}).items():
                    func_prop = FunctionProposal(
                        function_name=func_data["function_name"],
                        recommended_strategy=func_data["recommended_strategy"],
                        loop_var_name=func_data.get("loop_var_name"),
                        iterable_origin=func_data.get("iterable_origin"),
                        accumulator_mutated=func_data.get("accumulator_mutated", False),
                        evidence=func_data.get("evidence", []),
                    )
                    class_prop.add_function_proposal(func_prop)

                module_prop.add_class_proposal(class_prop)

            decision_model.add_module_proposal(module_prop)

        return decision_model

    except Exception as e:
        print(f"[ERROR] Failed to load decision model from {file_path}: {e}")
        return None


def print_detailed_analysis(decision_model: DecisionModel) -> None:
    """Print detailed analysis of the decision model."""
    print("[ANALYSIS] DECISION MODEL DETAILED ANALYSIS")
    print("=" * 60)

    stats = decision_model.get_statistics()
    print("[STATS] STATISTICS:")
    print(f"   Total modules: {stats['total_modules']}")
    print(f"   Total classes: {stats['total_classes']}")
    print(f"   Total functions: {stats['total_functions']}")
    print(f"   Evidence-rich functions: {stats['evidence_rich_functions']}")
    print(f"   Accumulator patterns detected: {stats['accumulator_detected']}")

    print("\n[STRATEGY] STRATEGY DISTRIBUTION:")
    for strategy, count in stats["strategy_counts"].items():
        percentage = (count / stats["total_functions"] * 100) if stats["total_functions"] > 0 else 0
        print(f"   {strategy}: {count} ({percentage:.1f}%)")

    # Validation results
    print("\n[VALIDATION] VALIDATION RESULTS:")
    warnings = decision_model.validate()
    if warnings:
        print(f"   [WARN] {len(warnings)} warnings found:")
        for warning in warnings[:10]:  # Show first 10 warnings
            print(f"      • {warning}")
        if len(warnings) > 10:
            print(f"      ... and {len(warnings) - 10} more")
    else:
        print("   [OK] No validation warnings")

    # Conflict detection
    print("\n[CONFLICTS] CONFLICT DETECTION:")
    conflicts = decision_model.detect_conflicts()
    if conflicts:
        print(f"   [CONFLICT] {len(conflicts)} conflicts found:")
        for conflict in conflicts[:10]:  # Show first 10 conflicts
            print(f"      • {conflict}")
        if len(conflicts) > 10:
            print(f"      ... and {len(conflicts) - 10} more")
    else:
        print("   [OK] No conflicts detected")


def print_function_details(decision_model: DecisionModel) -> None:
    """Print detailed information about each function."""
    print("\n[FUNCTIONS] FUNCTION DETAILS:")
    print("-" * 60)

    for module_name, module_prop in decision_model.module_proposals.items():
        print(f"\n[MODULE] Module: {Path(module_name).name}")

        for class_name, class_prop in module_prop.class_proposals.items():
            print(f"\n   [CLASS] Class: {class_name}")

            for func_name, func_prop in class_prop.function_proposals.items():
                print(f"\n      [FUNC] Function: {Path(func_name).name}")
                print(f"         Strategy: {func_prop.recommended_strategy}")
                print(f"         Loop variable: {func_prop.loop_var_name or 'None'}")
                print(f"         Iterable origin: {func_prop.iterable_origin or 'None'}")
                print(f"         Accumulator mutated: {func_prop.accumulator_mutated}")
                print("         Evidence:")
                for i, evidence in enumerate(func_prop.evidence, 1):
                    print(f"           {i}. {evidence}")


def analyze_file_and_show_model(file_path: Path, output_dir: Path | None = None) -> None:
    """Analyze a file and show the decision model."""
    print(f"[ANALYZE] Analyzing file: {file_path}")

    # Run the analysis
    from splurge_unittest_to_pytest.context import MigrationConfig, PipelineContext
    from splurge_unittest_to_pytest.events import EventBus
    from splurge_unittest_to_pytest.jobs.decision_analysis_job import DecisionAnalysisJob

    try:
        with open(file_path, encoding="utf-8") as f:
            source_code = f.read()

        event_bus = EventBus()
        job = DecisionAnalysisJob(event_bus)

        context = PipelineContext(
            source_file=str(file_path),
            target_file=None,
            config=MigrationConfig(enable_decision_analysis=True),
            run_id=f"debug-{file_path.stem}",
            metadata={},
        )

        result = job.execute(context, source_code)

        if result.is_success():
            decision_model = result.data
            print("[OK] Analysis completed successfully")

            # Show the results
            print_detailed_analysis(decision_model)
            print_function_details(decision_model)

            # Save if output directory specified
            if output_dir:
                output_file = output_dir / f"{file_path.stem}_debug.json"
                output_dir.mkdir(parents=True, exist_ok=True)

                # Convert to dict for JSON serialization
                data = {
                    "module_proposals": {
                        name: {
                            "module_name": prop.module_name,
                            "class_proposals": {
                                class_name: {
                                    "class_name": class_prop.class_name,
                                    "function_proposals": {
                                        func_name: {
                                            "function_name": func_prop.function_name,
                                            "recommended_strategy": func_prop.recommended_strategy,
                                            "loop_var_name": func_prop.loop_var_name,
                                            "iterable_origin": func_prop.iterable_origin,
                                            "accumulator_mutated": func_prop.accumulator_mutated,
                                            "evidence": func_prop.evidence,
                                        }
                                        for func_name, func_prop in class_prop.function_proposals.items()
                                    },
                                    "class_fixtures": class_prop.class_fixtures,
                                    "class_setup_methods": class_prop.class_setup_methods,
                                }
                                for class_name, class_prop in prop.class_proposals.items()
                            },
                            "module_fixtures": prop.module_fixtures,
                            "module_imports": prop.module_imports,
                            "top_level_assignments": prop.top_level_assignments,
                        }
                        for name, prop in decision_model.module_proposals.items()
                    }
                }

                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)

                print(f"[SAVED] Debug output saved to: {output_file}")

        else:
            print(f"[ERROR] Analysis failed: {result.error}")

    except Exception as e:
        print(f"[ERROR] Error during analysis: {e}")


def main():
    """Main entry point for the debug tool."""
    parser = argparse.ArgumentParser(description="Debug and introspect decision models")
    parser.add_argument("model_file", nargs="?", type=Path, help="Decision model JSON file to load and analyze")
    parser.add_argument("--analyze", type=Path, help="Python file to analyze and show decision model")
    parser.add_argument("--output-dir", type=Path, help="Directory to save debug output")

    args = parser.parse_args()

    if args.model_file:
        # Load and analyze existing decision model
        decision_model = load_decision_model(args.model_file)

        if decision_model:
            print_detailed_analysis(decision_model)
            print_function_details(decision_model)
        else:
            print("[ERROR] Failed to load decision model")
            return 1

    elif args.analyze:
        # Analyze a file and show the decision model
        analyze_file_and_show_model(args.analyze, args.output_dir)

    else:
        print("[ERROR] No action specified. Use --model-file to analyze existing model or --analyze to analyze a file")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
